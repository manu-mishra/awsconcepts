{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenizer Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in c:\\programdata\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.11.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (22.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: boto3 in c:\\users\\manumishra\\appdata\\roaming\\python\\python310\\site-packages (1.26.153)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.153 in c:\\users\\manumishra\\appdata\\roaming\\python\\python310\\site-packages (from boto3) (1.29.153)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\manumishra\\appdata\\roaming\\python\\python310\\site-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.153->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.30.0,>=1.29.153->boto3) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.153->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]\n",
    "%pip install boto3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import boto3\n",
    "\n",
    "# Specify the name of your S3 bucket and the file in the bucket\n",
    "bucket_name = \"awsc.datascience.objecjstore\"\n",
    "file_name = \"training_data/TokenTrainingText.txt\"\n",
    "local_file_name = \"TokenTrainingText.txt\"\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the file from the S3 bucket to the local file system in the SageMaker instance\n",
    "# '/tmp/your-file-name.txt' is the location and file name where the file will be downloaded\n",
    "s3.download_file(bucket_name, file_name, local_file_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrain with domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\awsblogs-vocab.json', '.\\\\awsblogs-merges.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tokenizer of the Byte Pair Encoding type\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Train the tokenizer on the downloaded file\n",
    "# vocab_size=52_000 and min_frequency=2 are hyperparameters that can be adjusted according to the specific characteristics of your text\n",
    "# special_tokens is a list of tokens that will be added to the tokenizer's vocabulary\n",
    "tokenizer.train(files=local_file_name, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Save the trained tokenizer to disk\n",
    "# It will create two files: 'aws-blogs-vocab.json' and 'aws-blogs-merges.txt'\n",
    "tokenizer.save_model(\".\", \"awsblogs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained tokenizer tokens:  ['aw', '##s', 'is', 'awesome', '.']\n",
      "Custom tokenizer tokens:  ['aws', 'Ġis', 'Ġawesome', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "\n",
    "# Define a test sentence\n",
    "test_sentence = \"aws is awesome.\"\n",
    "\n",
    "# Load the pre-trained tokenizer\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the test sentence with the pre-trained tokenizer\n",
    "pretrained_output = pretrained_tokenizer.tokenize(test_sentence)\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Pre-trained tokenizer tokens: \", pretrained_output)\n",
    "\n",
    "# Load the custom trained tokenizer\n",
    "custom_tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./awsblogs-vocab.json\",\n",
    "    \"./awsblogs-merges.txt\",\n",
    ")\n",
    "\n",
    "# Tokenize the test sentence with the custom tokenizer\n",
    "custom_output = custom_tokenizer.encode(test_sentence)\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Custom tokenizer tokens: \", custom_output.tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manumishra\\source\\repos\\manu-mishra\\awsconcepts\\Workloads\\AwsConceptsApp\\DataScience\\notebooks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee12c5bea4ff47d2a08dd5152bedd1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manumishra\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cd1f8c1c354f44aa03b37e0be611e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0621059   0.07121446  0.03583737  0.15002523 -0.01553448  0.06262513\n",
      "   0.02541252  0.08205176  0.0336398  -0.03258981  0.04044851 -0.14505434\n",
      "   0.0204825   0.09314897  0.0913348   0.05756848  0.03054859 -0.01449592\n",
      "  -0.11808332  0.0449594  -0.06246282  0.10158909  0.01026067  0.06910552\n",
      "   0.05934934  0.00413021 -0.07368245 -0.07226573 -0.0789161   0.00210865\n",
      "  -0.03282139  0.04227065 -0.00086759 -0.02976679  0.074085   -0.0158735\n",
      "   0.08096281  0.04571429 -0.05945798 -0.03628789  0.09128615 -0.1272096\n",
      "   0.08767764  0.10240908  0.01987366  0.03109403 -0.00810728  0.22222985\n",
      "   0.03203924  0.01365474]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "import torch\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# Load the trained tokenizer\n",
    "tokenizer = RobertaTokenizerFast(\n",
    "    vocab_file=\"./awsblogs-vocab.json\", \n",
    "    merges_file=\"./awsblogs-merges.txt\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    sep_token=\"</s>\",\n",
    "    cls_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\"\n",
    ")\n",
    "\n",
    "# Specify the model name\n",
    "model_name = 'roberta-base'  # You should use a model that matches the tokenizer (RoBERTa in this case)\n",
    "\n",
    "# Load the model\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0621059   0.07121446  0.03583737  0.15002523 -0.01553448  0.06262513\n",
      "   0.02541252  0.08205176  0.0336398  -0.03258981  0.04044851 -0.14505434\n",
      "   0.0204825   0.09314897  0.0913348   0.05756848  0.03054859 -0.01449592\n",
      "  -0.11808332  0.0449594  -0.06246282  0.10158909  0.01026067  0.06910552\n",
      "   0.05934934  0.00413021 -0.07368245 -0.07226573 -0.0789161   0.00210865\n",
      "  -0.03282139  0.04227065 -0.00086759 -0.02976679  0.074085   -0.0158735\n",
      "   0.08096281  0.04571429 -0.05945798 -0.03628789  0.09128615 -0.1272096\n",
      "   0.08767764  0.10240908  0.01987366  0.03109403 -0.00810728  0.22222985\n",
      "   0.03203924  0.01365474]]\n"
     ]
    }
   ],
   "source": [
    "# Define a sample text\n",
    "text = \"Hello, this is a test.\"\n",
    "\n",
    "# Encode the text to get the input tensors using your custom tokenizer\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Run the text through the model to get the embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Use the average of the last hidden state as the text's embedding\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "vectors = embeddings.detach().numpy()\n",
    "\n",
    "print(vectors[:, :50])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
